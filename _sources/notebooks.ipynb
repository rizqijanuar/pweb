{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Perangkingan Kalimat Berita dengan Method Page Rank"],"metadata":{"id":"NskBsDaKYWc4"}},{"cell_type":"markdown","source":["**Penjelasan Scrapy**\n","\n","Scrapy adalah web crawling dan web scraping framework tingkat tinggi yang cepat, digunakan untuk merayapi situs web dan mengekstrak data terstruktur dari halaman mereka. Ini dapat digunakan untuk berbagai tujuan, mulai dari penambangan data hingga pemantauan dan pengujian otomatis."],"metadata":{"id":"enJMHB4TYbmx"}},{"cell_type":"code","source":["!pip install scrapy\n","!pip install crochet"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LcwCDr2QYa94","outputId":"5faed629-94bc-4aab-e912-d6d1a5441d86","executionInfo":{"status":"ok","timestamp":1669762536458,"user_tz":-420,"elapsed":27190,"user":{"displayName":"r januaarr","userId":"17724389291269930399"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting scrapy\n","  Downloading Scrapy-2.7.1-py2.py3-none-any.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 12.3 MB/s \n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from scrapy) (57.4.0)\n","Collecting itemloaders>=1.0.1\n","  Downloading itemloaders-1.0.6-py3-none-any.whl (11 kB)\n","Collecting tldextract\n","  Downloading tldextract-3.4.0-py3-none-any.whl (93 kB)\n","\u001b[K     |████████████████████████████████| 93 kB 1.1 MB/s \n","\u001b[?25hCollecting Twisted>=18.9.0\n","  Downloading Twisted-22.10.0-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 90.6 MB/s \n","\u001b[?25hCollecting w3lib>=1.17.0\n","  Downloading w3lib-2.1.0-py3-none-any.whl (21 kB)\n","Collecting PyDispatcher>=2.0.5\n","  Downloading PyDispatcher-2.0.6.tar.gz (38 kB)\n","Collecting parsel>=1.5.0\n","  Downloading parsel-1.7.0-py2.py3-none-any.whl (14 kB)\n","Collecting pyOpenSSL>=21.0.0\n","  Downloading pyOpenSSL-22.1.0-py3-none-any.whl (57 kB)\n","\u001b[K     |████████████████████████████████| 57 kB 2.3 MB/s \n","\u001b[?25hCollecting cryptography>=3.3\n","  Downloading cryptography-38.0.4-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n","\u001b[K     |████████████████████████████████| 4.0 MB 43.2 MB/s \n","\u001b[?25hCollecting queuelib>=1.4.2\n","  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n","Collecting service-identity>=18.1.0\n","  Downloading service_identity-21.1.0-py2.py3-none-any.whl (12 kB)\n","Collecting cssselect>=0.9.1\n","  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n","Collecting itemadapter>=0.1.0\n","  Downloading itemadapter-0.7.0-py3-none-any.whl (10 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from scrapy) (21.3)\n","Collecting zope.interface>=5.1.0\n","  Downloading zope.interface-5.5.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (254 kB)\n","\u001b[K     |████████████████████████████████| 254 kB 65.2 MB/s \n","\u001b[?25hRequirement already satisfied: lxml>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (4.9.1)\n","Collecting protego>=0.1.15\n","  Downloading Protego-0.2.1-py2.py3-none-any.whl (8.2 kB)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=3.3->scrapy) (1.15.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=3.3->scrapy) (2.21)\n","Collecting jmespath>=0.9.5\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from protego>=0.1.15->scrapy) (1.15.0)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from service-identity>=18.1.0->scrapy) (22.1.0)\n","Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.7/dist-packages (from service-identity>=18.1.0->scrapy) (0.2.8)\n","Requirement already satisfied: pyasn1 in /usr/local/lib/python3.7/dist-packages (from service-identity>=18.1.0->scrapy) (0.4.8)\n","Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from Twisted>=18.9.0->scrapy) (4.1.1)\n","Collecting Automat>=0.8.0\n","  Downloading Automat-22.10.0-py2.py3-none-any.whl (26 kB)\n","Collecting incremental>=21.3.0\n","  Downloading incremental-22.10.0-py2.py3-none-any.whl (16 kB)\n","Collecting constantly>=15.1\n","  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n","Collecting hyperlink>=17.1.1\n","  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n","\u001b[K     |████████████████████████████████| 74 kB 958 kB/s \n","\u001b[?25hRequirement already satisfied: idna>=2.5 in /usr/local/lib/python3.7/dist-packages (from hyperlink>=17.1.1->Twisted>=18.9.0->scrapy) (2.10)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->scrapy) (3.0.9)\n","Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tldextract->scrapy) (2.23.0)\n","Collecting requests-file>=1.4\n","  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n","Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract->scrapy) (3.8.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->scrapy) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->scrapy) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2022.9.24)\n","Building wheels for collected packages: PyDispatcher\n","  Building wheel for PyDispatcher (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.6-py3-none-any.whl size=11959 sha256=f1b0c72b0fe67e67c53a4a9d0ad47bb54fb383c8d28bd1f93a533b84066cdf23\n","  Stored in directory: /root/.cache/pip/wheels/c9/d6/6a/de198d890277cde60ca3dbebe7ae592d3b381c7d9bb2455f4d\n","Successfully built PyDispatcher\n","Installing collected packages: w3lib, cssselect, zope.interface, requests-file, parsel, jmespath, itemadapter, incremental, hyperlink, cryptography, constantly, Automat, Twisted, tldextract, service-identity, queuelib, pyOpenSSL, PyDispatcher, protego, itemloaders, scrapy\n","Successfully installed Automat-22.10.0 PyDispatcher-2.0.6 Twisted-22.10.0 constantly-15.1.0 cryptography-38.0.4 cssselect-1.2.0 hyperlink-21.0.0 incremental-22.10.0 itemadapter-0.7.0 itemloaders-1.0.6 jmespath-1.0.1 parsel-1.7.0 protego-0.2.1 pyOpenSSL-22.1.0 queuelib-1.6.2 requests-file-1.5.1 scrapy-2.7.1 service-identity-21.1.0 tldextract-3.4.0 w3lib-2.1.0 zope.interface-5.5.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting crochet\n","  Downloading crochet-2.0.0-py3-none-any.whl (31 kB)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.7/dist-packages (from crochet) (1.14.1)\n","Requirement already satisfied: Twisted>=16.0 in /usr/local/lib/python3.7/dist-packages (from crochet) (22.10.0)\n","Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.7/dist-packages (from Twisted>=16.0->crochet) (15.1.0)\n","Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from Twisted>=16.0->crochet) (22.1.0)\n","Requirement already satisfied: zope.interface>=4.4.2 in /usr/local/lib/python3.7/dist-packages (from Twisted>=16.0->crochet) (5.5.2)\n","Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.7/dist-packages (from Twisted>=16.0->crochet) (21.0.0)\n","Requirement already satisfied: Automat>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from Twisted>=16.0->crochet) (22.10.0)\n","Requirement already satisfied: incremental>=21.3.0 in /usr/local/lib/python3.7/dist-packages (from Twisted>=16.0->crochet) (22.10.0)\n","Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.7/dist-packages (from Twisted>=16.0->crochet) (4.1.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from Automat>=0.8.0->Twisted>=16.0->crochet) (1.15.0)\n","Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.7/dist-packages (from hyperlink>=17.1.1->Twisted>=16.0->crochet) (2.10)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from zope.interface>=4.4.2->Twisted>=16.0->crochet) (57.4.0)\n","Installing collected packages: crochet\n","Successfully installed crochet-2.0.0\n"]}]},{"cell_type":"code","source":["import scrapy"],"metadata":{"id":"IKfg4bmlXDb9","executionInfo":{"status":"ok","timestamp":1669762537230,"user_tz":-420,"elapsed":789,"user":{"displayName":"r januaarr","userId":"17724389291269930399"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import scrapy\n","from scrapy.crawler import CrawlerRunner\n","import re\n","from crochet import setup, wait_for\n","setup()\n","\n","class QuotesToCsv(scrapy.Spider):\n","    name = \"MJKQuotesToCsv\"\n","    start_urls = [\n","        'https://nasional.tempo.co/read/1643890/jokowi-lantik-heru-budi-hartono-gantikan-anies-baswedan-senin-pekan-depan',\n","    ]\n","    custom_settings = {\n","        'ITEM_PIPELINES': {\n","            '__main__.ExtractFirstLine': 1\n","        },\n","        'FEEDS': {\n","            'news.csv': {\n","                'format': 'csv',\n","                'overwrite': True\n","            }\n","        }\n","    }\n","\n","    def parse(self, response):\n","        \"\"\"parse data from urls\"\"\"\n","        for quote in response.css('#isi > p'):\n","            yield {'news': quote.extract()}\n","\n","\n","class ExtractFirstLine(object):\n","    def process_item(self, item, spider):\n","        \"\"\"text processing\"\"\"\n","        lines = dict(item)[\"news\"].splitlines()\n","        first_line = self.__remove_html_tags__(lines[0])\n","\n","        return {'news': first_line}\n","\n","    def __remove_html_tags__(self, text):\n","        \"\"\"remove html tags from string\"\"\"\n","        html_tags = re.compile('<.*?>')\n","        return re.sub(html_tags, '', text)\n","\n","@wait_for(10)\n","def run_spider():\n","    \"\"\"run spider with MJKQuotesToCsv\"\"\"\n","    crawler = CrawlerRunner()\n","    d = crawler.crawl(QuotesToCsv)\n","    return d"],"metadata":{"id":"rIW1IWi6Yl-n","executionInfo":{"status":"ok","timestamp":1669762537232,"user_tz":-420,"elapsed":11,"user":{"displayName":"r januaarr","userId":"17724389291269930399"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["run_spider()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rua3pL8JY6dW","outputId":"959df3e5-0c69-46a3-9ab7-0cd218c22055","executionInfo":{"status":"ok","timestamp":1669762539183,"user_tz":-420,"elapsed":1961,"user":{"displayName":"r januaarr","userId":"17724389291269930399"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:twisted:/usr/local/lib/python3.7/dist-packages/scrapy/utils/request.py:231: scrapy.exceptions.ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n","\n","It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n","\n","See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n"]}]},{"cell_type":"markdown","source":["Mengambil dan Membaca data CSV yang bernama news.csv"],"metadata":{"id":"0_ZxsB-NZEIw"}},{"cell_type":"code","source":["dataNews = pd.read_csv('news.csv')\n","dataNews"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":183},"id":"qGChPD10Y9L-","outputId":"5fd19805-2274-43ef-b30a-dd1c739eb0fa","executionInfo":{"status":"error","timestamp":1669762539184,"user_tz":-420,"elapsed":48,"user":{"displayName":"r januaarr","userId":"17724389291269930399"}}},"execution_count":5,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-e1a3d7c03a59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataNews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'news.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdataNews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"]}]},{"cell_type":"markdown","source":["PyPDF2 adalah pustaka PDF python murni gratis dan open-source yang mampu memisahkan, menggabungkan , memotong, dan mengubah halaman file PDF.\n","\n","Install PyPDF2"],"metadata":{"id":"MIVazGhtZHM7"}},{"cell_type":"code","source":["!pip install PyPDF2"],"metadata":{"id":"7p7IO54CZCCW","executionInfo":{"status":"aborted","timestamp":1669762539188,"user_tz":-420,"elapsed":47,"user":{"displayName":"r januaarr","userId":"17724389291269930399"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import PyPDF2"],"metadata":{"id":"wwCjFN0hZKvW","executionInfo":{"status":"aborted","timestamp":1669762539189,"user_tz":-420,"elapsed":47,"user":{"displayName":"r januaarr","userId":"17724389291269930399"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Membaca Pdf dari file lalu dibuat menjadi bentuk document Text"],"metadata":{"id":"YSSKFzbzZZcP"}},{"cell_type":"code","source":["pdfReader = PyPDF2.PdfFileReader('/content/drive/MyDrive/webmining/news.pdf')\n","pageObj = pdfReader.getPage(0)\n","document = pageObj.extractText()\n","document"],"metadata":{"id":"8GqzRIlHZMGF","executionInfo":{"status":"aborted","timestamp":1669762539190,"user_tz":-420,"elapsed":47,"user":{"displayName":"r januaarr","userId":"17724389291269930399"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["PunktSentenceTokenizer adalah Sebuah tokenizer kalimat yang menggunakan algoritma tanpa pengawasan untuk membangun model untuk kata-kata singkatan, kolokasi, dan kata-kata yang memulai kalimat dan kemudian menggunakan model itu untuk menemukan batas kalimat."],"metadata":{"id":"tgTRd0RrZbkn"}},{"cell_type":"code","source":["from nltk.tokenize.punkt import PunktSentenceTokenizer"],"metadata":{"id":"YRvOfTAwZW6m","executionInfo":{"status":"aborted","timestamp":1669762539191,"user_tz":-420,"elapsed":48,"user":{"displayName":"r januaarr","userId":"17724389291269930399"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def tokenize(document):\n","    # Kita memecahnya menggunakan  PunktSentenceTokenizer\n","    doc_tokenizer = PunktSentenceTokenizer()\n","    # sentences_list adalah daftar masing masing kalimat dari dokumen yang ada.\n","    sentences_list = doc_tokenizer.tokenize(document)\n","    return sentences_list"],"metadata":{"id":"0HOaXIutZc_m","executionInfo":{"status":"aborted","timestamp":1669762539192,"user_tz":-420,"elapsed":48,"user":{"displayName":"r januaarr","userId":"17724389291269930399"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentences_list = tokenize(document)\n","sentences_list"],"metadata":{"id":"E7N0VIJvZd9u","executionInfo":{"status":"aborted","timestamp":1669762539193,"user_tz":-420,"elapsed":48,"user":{"displayName":"r januaarr","userId":"17724389291269930399"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Merapikan data di atas sehingga lebih enak dibaca"],"metadata":{"id":"QZEf1mrbZhA-"}},{"cell_type":"code","source":["kal=1\n","for i in sentences_list:\n","    print('\\nKalimat {}'.format(kal))\n","    kal+=1\n","    print(i)"],"metadata":{"id":"C9mAG7VvZfTI","executionInfo":{"status":"aborted","timestamp":1669762539195,"user_tz":-420,"elapsed":49,"user":{"displayName":"r januaarr","userId":"17724389291269930399"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Tokenizing adalah proses pemisahan teks menjadi potongan-potongan yang disebut sebagai token untuk kemudian di analisa. Kata, angka, simbol, tanda baca dan entitas penting lainnya dapat dianggap sebagai token."],"metadata":{"id":"Ze9Q1X_yZlF0"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n","vectorizer = CountVectorizer()\n","cv_matrix=vectorizer.fit_transform(sentences_list)"],"metadata":{"id":"ppaD7XLUZiVW","executionInfo":{"status":"aborted","timestamp":1669762539198,"user_tz":-420,"elapsed":52,"user":{"displayName":"r januaarr","userId":"17724389291269930399"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Menampilkan jumlah Kosa Kata dari Data"],"metadata":{"id":"1w8vRcbiZo0c"}},{"cell_type":"code","source":["print (\"Banyaknya kosa kata = \", len((vectorizer.get_feature_names_out())))"],"metadata":{"id":"yxKT2OAdZmhO","executionInfo":{"status":"aborted","timestamp":1669762539200,"user_tz":-420,"elapsed":53,"user":{"displayName":"r januaarr","userId":"17724389291269930399"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Menampilkan jumlah Kalimat dari Data"],"metadata":{"id":"M9JAIAZJZrjC"}},{"cell_type":"code","source":["print (\"Banyaknya kalimat = \", (len(sentences_list)))"],"metadata":{"id":"9AxYl96yZp_e","executionInfo":{"status":"aborted","timestamp":1669762539201,"user_tz":-420,"elapsed":53,"user":{"displayName":"r januaarr","userId":"17724389291269930399"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Menampilkan Kosa Kata dari Data\n","\n"],"metadata":{"id":"YNVDFt7eZw6X"}},{"cell_type":"code","source":["print (\"kosa kata = \", (vectorizer.get_feature_names_out()))"],"metadata":{"id":"ys5W2ZvxZyXQ","executionInfo":{"status":"aborted","timestamp":1669762539202,"user_tz":-420,"elapsed":54,"user":{"displayName":"r januaarr","userId":"17724389291269930399"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# mengubah kumpulan dokumen mentah menjadi matriks fitur TF-IDF\n","normal_matrix = TfidfTransformer().fit_transform(cv_matrix)\n","print(normal_matrix.toarray())"],"metadata":{"id":"cR1Nu11-ZzC2","executionInfo":{"status":"aborted","timestamp":1669762539203,"user_tz":-420,"elapsed":54,"user":{"displayName":"r januaarr","userId":"17724389291269930399"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Menampilkan Jumlah Kalimat dan Kosa Kata"],"metadata":{"id":"eW9Y8M29Z3wx"}},{"cell_type":"code","source":["normal_matrix.shape"],"metadata":{"id":"r71WU9j2Z1Pe","executionInfo":{"status":"aborted","timestamp":1669762539204,"user_tz":-420,"elapsed":54,"user":{"displayName":"r januaarr","userId":"17724389291269930399"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["NetworkX adalah paket Python untuk pembuatan, manipulasi, dan studi tentang struktur, dinamika, dan fungsi jaringan yang kompleks. Ini menyediakan:"],"metadata":{"id":"WTW-PPjoZ62_"}},{"cell_type":"code","source":["import networkx as nx"],"metadata":{"id":"H9dzm8iuZ45n","executionInfo":{"status":"aborted","timestamp":1669762539206,"user_tz":-420,"elapsed":56,"user":{"displayName":"r januaarr","userId":"17724389291269930399"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Graph adalah kumpulan dati titik (node) dan garis dimana pasangan – pasangan titik (node) tersebut dihubungkan oleh segmen garis. Node ini biasa disebut simpul (vertex) dan segmen garis disebut ruas (edge)"],"metadata":{"id":"LK-1uogaZ-My"}},{"cell_type":"code","source":["res_graph = normal_matrix * normal_matrix.T\n","print(res_graph)"],"metadata":{"id":"iZQzEtSsZ8bW","executionInfo":{"status":"aborted","timestamp":1669762539208,"user_tz":-420,"elapsed":57,"user":{"displayName":"r januaarr","userId":"17724389291269930399"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nx_graph = nx.from_scipy_sparse_matrix(res_graph)"],"metadata":{"id":"qfYbsT2EaEbF","executionInfo":{"status":"aborted","timestamp":1669762539209,"user_tz":-420,"elapsed":58,"user":{"displayName":"r januaarr","userId":"17724389291269930399"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nx.draw_circular(nx_graph)"],"metadata":{"id":"AKTDgUkHaGZW","executionInfo":{"status":"aborted","timestamp":1669762539209,"user_tz":-420,"elapsed":57,"user":{"displayName":"r januaarr","userId":"17724389291269930399"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Jumlah Banyak Sisi"],"metadata":{"id":"SnCxMVamaJXZ"}},{"cell_type":"code","source":["print('Banyaknya sisi {}'.format(nx_graph.number_of_edges()))"],"metadata":{"id":"8YAyb3MRaHTN","executionInfo":{"status":"aborted","timestamp":1669762539210,"user_tz":-420,"elapsed":58,"user":{"displayName":"r januaarr","userId":"17724389291269930399"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Menkalikan data dengan data Transpose"],"metadata":{"id":"fpbH9w2yaMkn"}},{"cell_type":"code","source":["res_graph = normal_matrix * normal_matrix.T"],"metadata":{"id":"gtZKj8Q1aK21","executionInfo":{"status":"aborted","timestamp":1669762539211,"user_tz":-420,"elapsed":58,"user":{"displayName":"r januaarr","userId":"17724389291269930399"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["PageRank menghitung peringkat node dalam grafik G berdasarkan struktur tautan masuk. Awalnya dirancang sebagai algoritma untuk menentukan peringkat halaman web."],"metadata":{"id":"ZOiMdABRaQYI"}},{"cell_type":"code","source":["ranks=nx.pagerank(nx_graph,)"],"metadata":{"id":"FicEKXXMaPNl","executionInfo":{"status":"aborted","timestamp":1669762539212,"user_tz":-420,"elapsed":58,"user":{"displayName":"r januaarr","userId":"17724389291269930399"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["memasukkan data ke array"],"metadata":{"id":"vYefNZvSaT9F"}},{"cell_type":"code","source":["arrRank=[]\n","for i in ranks:\n","    arrRank.append(ranks[i])"],"metadata":{"id":"kdMeYGY7aRxl","executionInfo":{"status":"aborted","timestamp":1669762539212,"user_tz":-420,"elapsed":58,"user":{"displayName":"r januaarr","userId":"17724389291269930399"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["menjadikan data kedalam bentuk tabel lalu digabungkan"],"metadata":{"id":"fa4qJbnraWjU"}},{"cell_type":"code","source":["dfRanks = pd.DataFrame(arrRank,columns=['PageRank'])\n","dfSentence = pd.DataFrame(sentences_list,columns=['News'])\n","dfJoin = pd.concat([dfSentence,dfRanks], axis=1)\n","dfJoin"],"metadata":{"id":"xFt3sVwHaVTN","executionInfo":{"status":"aborted","timestamp":1669762539213,"user_tz":-420,"elapsed":59,"user":{"displayName":"r januaarr","userId":"17724389291269930399"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Mengurutkan data berdasarkan hasil tertinggi"],"metadata":{"id":"zq2WxPWyaa2v"}},{"cell_type":"code","source":["sortSentence=dfJoin.sort_values(by=['PageRank'],ascending=False)\n","sortSentence"],"metadata":{"id":"yVrZwXPEaX7N","executionInfo":{"status":"aborted","timestamp":1669762539217,"user_tz":-420,"elapsed":62,"user":{"displayName":"r januaarr","userId":"17724389291269930399"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Menampilkan data dari 5 ke atas"],"metadata":{"id":"Fw3-3sDXad24"}},{"cell_type":"code","source":["sortSentence.head(5)"],"metadata":{"id":"HMHEQAMkacEG","executionInfo":{"status":"aborted","timestamp":1669762539219,"user_tz":-420,"elapsed":64,"user":{"displayName":"r januaarr","userId":"17724389291269930399"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Latent Semantic Indexing(LSI) Topik Berita"],"metadata":{"id":"NTTa5sKZakDV"}},{"cell_type":"code","source":["!pip install nltk"],"metadata":{"id":"P3rYfCZDafh9","executionInfo":{"status":"aborted","timestamp":1669762539220,"user_tz":-420,"elapsed":65,"user":{"displayName":"r januaarr","userId":"17724389291269930399"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install PySastrawi"],"metadata":{"id":"SVFB6ExbamY1","executionInfo":{"status":"aborted","timestamp":1669762539221,"user_tz":-420,"elapsed":65,"user":{"displayName":"r januaarr","userId":"17724389291269930399"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install Sastrawi"],"metadata":{"id":"49w9-Xqaann4","executionInfo":{"status":"aborted","timestamp":1669762539222,"user_tz":-420,"elapsed":66,"user":{"displayName":"r januaarr","userId":"17724389291269930399"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import PyPDF2"],"metadata":{"id":"idODvf_tapkW","executionInfo":{"status":"aborted","timestamp":1669762539225,"user_tz":-420,"elapsed":68,"user":{"displayName":"r januaarr","userId":"17724389291269930399"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pdfReader = PyPDF2.PdfFileReader('/content/drive/MyDrive/webmining/news.pdf')\n","pageObj = pdfReader.getPage(0)\n","document = pageObj.extractText()\n","print(document)"],"metadata":{"id":"y42e4_5Yaq29","executionInfo":{"status":"aborted","timestamp":1669762539226,"user_tz":-420,"elapsed":69,"user":{"displayName":"r januaarr","userId":"17724389291269930399"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import re\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import nltk\n","nltk.download('stopwords')"],"metadata":{"id":"YcnJ1cfva1Sl","executionInfo":{"status":"aborted","timestamp":1669762539226,"user_tz":-420,"elapsed":69,"user":{"displayName":"r januaarr","userId":"17724389291269930399"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["word_tokens = word_tokenize(document)\n","print(word_tokens)"],"metadata":{"id":"rpU4D_Lya3wt","executionInfo":{"status":"aborted","timestamp":1669762539227,"user_tz":-420,"elapsed":69,"user":{"displayName":"r januaarr","userId":"17724389291269930399"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from nltk.tokenize import RegexpTokenizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import TruncatedSVD"],"metadata":{"id":"4QFsizHaa5GF","executionInfo":{"status":"aborted","timestamp":1669762539227,"user_tz":-420,"elapsed":69,"user":{"displayName":"r januaarr","userId":"17724389291269930399"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["stop_words = set(stopwords.words('indonesian'))\n","word_tokens_no_stopwords = [w for w in word_tokens if not w in stop_words]\n","print(word_tokens_no_stopwords)"],"metadata":{"id":"sNP2upJPbTr2","executionInfo":{"status":"aborted","timestamp":1669762539228,"user_tz":-420,"elapsed":70,"user":{"displayName":"r januaarr","userId":"17724389291269930399"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Vectorize document using TF-IDF\n","tfidf = TfidfVectorizer(lowercase=True,\n","                        ngram_range = (1,1))\n","\n","# Fit and Transform the documents\n","train_data = tfidf.fit_transform(word_tokens_no_stopwords)\n","train_data"],"metadata":{"id":"yNac7i25a6yc","executionInfo":{"status":"aborted","timestamp":1669762539229,"user_tz":-420,"elapsed":70,"user":{"displayName":"r januaarr","userId":"17724389291269930399"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_components=10\n","\n","# Create SVD object\n","lsa = TruncatedSVD(n_components=num_components, n_iter=100, random_state=42)\n","\n","# Fit SVD model on data\n","lsa.fit_transform(train_data)\n","\n","# Get Singular values and Components \n","Sigma = lsa.singular_values_ \n","V_transpose = lsa.components_.T\n","V_transpose"],"metadata":{"id":"crlZffeMa8BA","executionInfo":{"status":"aborted","timestamp":1669762539229,"user_tz":-420,"elapsed":70,"user":{"displayName":"r januaarr","userId":"17724389291269930399"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Print the topics with their terms\n","terms = tfidf.get_feature_names()\n","\n","for index, component in enumerate(lsa.components_):\n","    zipped = zip(terms, component)\n","    top_terms_key=sorted(zipped, key = lambda t: t[1], reverse=True)[:5]\n","    top_terms_list=list(dict(top_terms_key).keys())\n","    print(\"Topic \"+str(index+1)+\": \",top_terms_list)"],"metadata":{"id":"WggnE8o1bYCO","executionInfo":{"status":"aborted","timestamp":1669762539230,"user_tz":-420,"elapsed":71,"user":{"displayName":"r januaarr","userId":"17724389291269930399"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"CrvaY0hMbZ1Z","executionInfo":{"status":"aborted","timestamp":1669762539230,"user_tz":-420,"elapsed":71,"user":{"displayName":"r januaarr","userId":"17724389291269930399"}}},"execution_count":null,"outputs":[]}]}